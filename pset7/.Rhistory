for (j in 1:k){
cat(j,'')
train.rows = dcc$fold != j
test.rows  = dcc$fold == j
# the models are already defined in question 9, so only need to predict
dcc$lm[test.rows] = predict(lm, newdata=dcc[test.rows,], type='response')
dcc$lmer[test.rows] = predict(lmer, newdata=dcc[test.rows,], type='response')
}
sqrt(mean((dcc$lm    - dcc$l2)^2, na.rm=T))
sqrt(mean((dcc$lmer  - dcc$l2)^2, na.rm=T))
roc.glm = roc(dcc$l2, dcc$lm)
roc.glmer = roc(dcc$l2, dcc$lmer)
sqrt(mean((dcc$lm    - dcc$l2)^2, na.rm=T))
sqrt(mean((dcc$lmer  - dcc$l2)^2, na.rm=T))
roc.glm = roc(dcc$l2, dcc$lm)
roc.glmer = roc(dcc$l2, dcc$lmer)
roc.glm$auc
roc.glmer$auc
e <- 7
d <- 4
(e==7)*d
e*d
(e!=7)*d
knitr::opts_chunk$set(echo = TRUE)
library(MASS, exclude = 'select')
library(lme4)
library(arm)
library(tidyverse)
library(pubtheme)
library(corrplot)
library(glmnet)
library(pROC)
load('data/labeled_points.Rdata')
labeled = labeled %>%
select(ID, landcover)
dl = labeled_train %>%
left_join(labeled, by = 'ID') %>%
mutate(veg = ifelse(landcover %in% c('natforest', 'orchard', 'cropland'),
1, 0),
NDVI100 = NDVI*100,
NDBI100 = NDBI*100)
dm = dl %>%
group_by(ID) %>%
summarise(
B1   = mean(B1, na.rm=T),
B2   = mean(B2, na.rm=T),
B3   = mean(B3, na.rm=T),
B4   = mean(B4, na.rm=T),
B5   = mean(B5, na.rm=T),
B6_VCID_1 = mean(B6_VCID_1, na.rm=T),
B6_VCID_2 = mean(B6_VCID_2, na.rm=T), ##  cor is .998 with B6_VCID_1
B7        = mean(B7, na.rm=T),
NDVI100   = mean(NDVI100, na.rm=T),
#NDBI100   = mean(NDBI100, na.rm=T), ## causes warnings with lasso
#EVI  = mean(EVI, na.rm=T),
landcover = unique(landcover),
veg = unique(veg)) %>%
select(-ID, -landcover)
ht(dm) ## head and tail, each with 2 rows
corr = dm %>%
cor(use = 'pairwise.complete.obs')
corr %>% round(2)
corrplot(corr, order = 'hclust', diag=T, type = 'upper', tl.pos = 'tp')
corrplot(corr, order = 'hclust', diag=F, type = 'lower', tl.pos =  'n',
method='number',
cl.pos = 'n',
add = T,
number.cex = 0.8)
corrplot(corr,
diag = F,
order = 'hclust',
addrect = 4)
## logistic regression with no regularization using only NDVI100
m1 = glm(veg ~ NDVI100, data = dm, family = binomial)
dm$predm1 <- predict(m1, type = 'response', newdata = dm)
summary(m1)
## logistic regression with no regularization using all band values
m2 = glm(veg ~ ., data = dm, family = binomial)
dm$predm2 <- predict(m2, type = 'response', newdata = dm)
summary(m2)
## logistic regression with ridge regularization using all band values
x = model.matrix(veg ~ B1 + B2 + B3 + B4 + B5 +
B6_VCID_1 + B6_VCID_2 + B7 + NDVI100, data = dm)[,-1]
y <- dm$veg
m3 <- cv.glmnet(x, y, family = 'binomial', alpha = 0)
dm$predm3 <- predict(m3, newx = x, s = 'lambda.1se', type = 'response')
summary(m3)
## logistic regression with lasso regularization using all band values
m4 <- cv.glmnet(x, y, family = 'binomial', alpha = 1)
dm$predm4 <- predict(m4, newx = x, s = 'lambda.1se', type = 'response')
summary(m4)
head(dm)
coefs.m3 = coef(m3, s = m3$lambda)
coefs.m4 = coef(m4, s = m4$lambda)
colnames(coefs.m3) = paste0('lambda', round(m3$lambda,6))
colnames(coefs.m4) = paste0('lambda', round(m4$lambda,6))
coefs.m3   <- coefs.m3   %>%
as.matrix() %>%
as.data.frame() %>%
rownames_to_column() %>%
pivot_longer(cols=-rowname) %>%
mutate(model='Ridge')
coefs.m4   <- coefs.m4   %>%
as.matrix() %>%
as.data.frame() %>%
rownames_to_column() %>%
pivot_longer(cols=-rowname) %>%
mutate(model='Lasso')
# bind rows
coefs1 = bind_rows(coefs.m3, coefs.m4) %>%
mutate(name = as.numeric(gsub('lambda', '', name))) %>%
filter(rowname!='(Intercept)') %>%
rename(lambda=name,
var = rowname)
head(coefs1)
lambda.lines = data.frame(model=c('Ridge', 'Lasso'),
lambda.min = c(m3$lambda.min, m4$lambda.min),
lambda.1se = c(m3$lambda.1se, m4$lambda.1se))
dg = coefs1
g = ggplot(data=dg, aes(x=lambda, y=value, group=var, color=var))+
geom_line(alpha=1, linewidth=1)+
facet_wrap(~model, ncol=1, scales='free_y')+
geom_vline(data=lambda.lines, aes(xintercept=lambda.min),
color = pubblue)+
geom_vline(data=lambda.lines, aes(xintercept=lambda.1se),
color = pubmediumgray)+
scale_x_log10()+
geom_hline(yintercept = 0)+
scale_color_manual(values=cb.pal) +
theme_bw() +
labs(title='Trace Curves', x='Lambda', y='Coefficient')
g
# for reproducibility
set.seed(123)
# set number of folds (usually 5 or 10)
k=5
# create a vector of fold numbers, repeating 1:k until reaching the number of rows in the data
folds = rep(1:k,
length.out = nrow(dm))
# create a new column in the data frame that randomly assigns a fold to each row without replacement
# (so that we don't have a row assigned to two or more folds)
dm$fold = sample(folds,
nrow(dm),
replace = F)
# view the first few rows of the data frame to see the fold assignments
head(dm)
# create a data frame to keep track of the metrics
metrics = data.frame(fold = 1:k,
llr1se = NA,
llrmin = NA,
lllas1se = NA,
lllasmin = NA,
llm1 = NA,
llm2 = NA)
# initialize the prediction columns
dm[, c('r.1se', 'las.1se', 'r.min', 'las.min', 'm1pred', 'm2pred')] = NA
# loop through the folds
for (j in 1:k){
cat(j,'')
## train rows are the ones not in the j-th fold
train.rows <- dm$fold != j
# test rows are the observations in the j-th fold
test.rows  <- dm$fold == j
## ======
## fit models to training data
## ======
## logistic regression with no regularization using only NDVI100
m1 = glm(veg ~ NDVI100, data = dm[train.rows, 1:10], family = "binomial")
## logistic regression with no regularization using all band values
m2 = glm(veg ~ ., data = dm[train.rows,1:10], family = "binomial")
## logistic regression with ridge regularization using all band values
r.train <- cv.glmnet(x = x[train.rows,], y = dm$veg[train.rows], family = 'binomial', alpha = 0)
## logistic regression with lasso regularization using all band values
las.train <- cv.glmnet(x = x[train.rows,], y = dm$veg[train.rows], family = 'binomial', alpha = 1)
## ======
## make predictions
## ======
dm$m1pred[test.rows] = predict(m1, newdata=dm[test.rows,], type='response')
dm$m2pred[test.rows] = predict(m2, newdata=dm[test.rows,], type='response')
dm$r.1se[test.rows] = predict(r.train, newx=x[test.rows,], s='lambda.1se', type='response')
dm$r.min[test.rows] = predict(r.train, newx=x[test.rows,], s='lambda.min', type='response')
dm$las.1se[test.rows] = predict(las.train, newx=x[test.rows,], s='lambda.1se', type='response')
dm$las.min[test.rows] = predict(las.train, newx=x[test.rows,], s='lambda.min', type='response')
## Test logloss for each fold
metrics[j,'llr1se'] = -mean(dm$veg[test.rows]*
log(dm$r.1se[test.rows]) +
(1-dm$veg[test.rows])*
log(1-dm$r.1se[test.rows]))
metrics[j,'llrmin'] = -mean(dm$veg[test.rows]*
log(dm$r.min[test.rows]) +
(1-dm$veg[test.rows])*
log(1-dm$r.min[test.rows]))
metrics[j,'lllas1se'] = -mean(dm$veg[test.rows]*
log(dm$las.1se[test.rows]) +
(1-dm$veg[test.rows])*
log(1-dm$las.1se[test.rows]))
metrics[j,'lllasmin'] = -mean(dm$veg[test.rows]*
log(dm$las.min[test.rows]) +
(1-dm$veg[test.rows])*
log(1-dm$las.min[test.rows]))
metrics[j,'llm1'] = -mean(dm$veg[test.rows]*
log(dm$m1pred[test.rows]) +
(1-dm$veg[test.rows])*
log(1-dm$m1pred[test.rows]))
metrics[j,'llm2'] = -mean(dm$veg[test.rows]*
log(dm$m2pred[test.rows]) +
(1-dm$veg[test.rows])*
log(1-dm$m2pred[test.rows]))
}
head(dm %>% select('r.1se', 'r.min', 'las.1se', 'las.min', 'm1pred', 'm2pred'))
metrics
logloss <- data.frame(m1 = NA,
m2 = NA,
m3 = NA,
m4 = NA,
m1out = NA,
m2out = NA,
r1se = NA,
rmin = NA,
las1se = NA,
lasmin = NA)
# Log loss of every model (in sample)
logloss$m1 <- (-mean(dm$veg*log(dm$predm1) + (1-dm$veg)*log(1-dm$predm1)))
logloss$m2 <- (-mean(dm$veg*log(dm$predm2) + (1-dm$veg)*log(1-dm$predm2)))
logloss$m3 <- (-mean(dm$veg*log(dm$predm3) + (1-dm$veg)*log(1-dm$predm3)))
logloss$m4 <- (-mean(dm$veg*log(dm$predm4) + (1-dm$veg)*log(1-dm$predm4)))
# Log Loss of every model (out of sample)
logloss$r1se <- (-mean(dm$veg*log(dm$r.1se) + (1-dm$veg)*log(1-dm$r.1se)))
logloss$rmin <- (-mean(dm$veg*log(dm$r.min) + (1-dm$veg)*log(1-dm$r.min)))
logloss$las1se <- (-mean(dm$veg*log(dm$las.1se) + (1-dm$veg)*log(1-dm$las.1se)))
logloss$lasmin <- (-mean(dm$veg*log(dm$las.min) + (1-dm$veg)*log(1-dm$las.min)))
logloss$m1out <- (-mean(dm$veg*log(dm$m1pred) + (1-dm$veg)*log(1-dm$m1pred)))
logloss$m2out <- (-mean(dm$veg*log(dm$m2pred) + (1-dm$veg)*log(1-dm$m2pred)))
logloss <- logloss %>%
pivot_longer(cols = everything(), names_to = 'model', values_to = 'logloss') %>%
mutate(sample = ifelse(model %in% c('m1', 'm2', 'm3', 'm4'), 'in', 'out'))
logloss
d = readRDS('data/games.rds')
tms = read.csv('data/nba.teams.csv')
d = d %>%
filter(lg=='nba', season %in% 2022, season.type=='reg') %>%
dplyr::select(date, away, home, ascore, hscore, season, gid)
da = d %>% dplyr::select(date, away, ascore, home, hscore, season, gid) %>% mutate(ha = 'away')
dh = d %>% dplyr::select(date, home, hscore, away, ascore, season, gid) %>% mutate(ha = 'home')
colnames(da) = c('date', 'team', 'score',  'opp', 'opp.score', 'season', 'gid', 'ha')
colnames(dh) = c('date', 'team', 'score',  'opp', 'opp.score', 'season', 'gid', 'ha')
d = bind_rows(da, dh) %>%
arrange(date, gid) %>%
left_join(tms %>% dplyr::select(team, div),
by = c('team'='team')) %>%
left_join(tms %>% dplyr::select(team, div),
by = c('opp'='team'), suffix=c('.team', '.opp'))
head(d)
extract.ranef = function(lmer.model=NULL, lm.model=NULL){
vars = names(ranef(lmer.model))
lmer.coefs = NULL
lm.coefs = NULL
if(!is.null(lm.model)){
lm.coefs = summary(lm.model)$coefficients %>%
as.data.frame() %>%
rownames_to_column(var='var') %>%
filter(grepl(paste0(vars, collapse="|"), var)) %>%
rename(est='Estimate',
se ='Std. Error') %>%
dplyr::select(var, est, se) %>%
mutate(model='glm')
}
for (j in vars){
## lmer
est = ranef(lmer.model)[[j]]
se  = se.ranef(lmer.model)[[j]]
colnames(est) = 'est'
colnames(se) = 'se'
temp = data.frame(var = rownames(est),
est=est,
se=se,
model='glmer',
ranef = j)
lmer.coefs = rbind(lmer.coefs, temp)
## lm
if(!is.null(lm.model)){
rows=grepl(j, lm.coefs$var)
lm.coefs$var[rows] = lm.model$xlevels[[j]][-length(lm.model$xlevels[[j]])]
lm.coefs$ranef[rows] = j
} ## if lm.model
} ## end j loop
coefs = rbind(lmer.coefs, lm.coefs)
return(coefs)
}
## linear regression model
lm1 <- lm(score ~ ha + team + opp, data = d, contrasts = list(team = 'contr.sum',
opp = 'contr.sum'))
## mixed effects linear regression model
lmer1 <- lmer(score ~ ha + (1|team) + (1|opp), data = d)
# extract coefficients
coefs = extract.ranef(lmer1, lm1)
head(coefs)
n.obs.team = d %>%
group_by(team) %>%
summarise(n = n(),
value = mean(score)) %>%
rename(var = team) %>%
mutate(ranef = 'team')
n.obs.team %>% head()
n.obs.opp = d %>%
group_by(opp) %>%
summarise(n = n(),
value = mean(score)) %>%
rename(var = opp) %>%
mutate(ranef = 'opp')
n.obs.opp %>% head()
n.obs = rbind(n.obs.team, n.obs.opp)
df = coefs %>%
pivot_wider(names_from  = model,
values_from = c(est, se),
names_sep   = '.') %>%
left_join(n.obs,
by = c('var', 'ranef'))
head(df)
tail(df)
dg2 = coefs %>%
arrange(model, est) %>%
mutate(var = factor(var, levels = unique(var))) %>%
left_join(n.obs,
by = c('var', 'ranef'))
g <- ggplot(df,
aes(x = est.glm,
y = est.glmer,
color = ranef,
label = var)) +
geom_abline(slope = 1,
intercept = 0) +
geom_hline(yintercept = 0) +
geom_segment(aes(xend = est.glm,
y = est.glmer + se.glmer,
yend = est.glmer - se.glmer),
color = pubblue,
linewidth = 0.25,
alpha = 0.3)+
geom_segment(aes(xend = est.glm,
y = est.glm + se.glm,
yend = est.glm - se.glm),
color = pubred,
linewidth = 0.25,
alpha = 0.3) +
geom_point(color = pubblue) +
geom_point(aes(y = est.glm),
color = pubred) +
geom_text_repel(hjust = -0.2,
size = 3,
color = pubmediumgray) +
facet_wrap(~ranef, nrow = 1) +
coord_cartesian(clip='off', expand=F)
g %>% pub()
gg = ggplot(dg2,
aes(y = var,
color = model))+
geom_vline(xintercept = 0,
color = pubmediumgray)+
geom_point(aes(x = est)) +
facet_wrap(~ranef) +
geom_segment(aes(x     = est + se,
xend  = est - se,
y     = var,
yend  = var),
linewidth = 0.5)
gg |> pub()
# for reproducibility
set.seed(123)
# set number of folds (usually 5 or 10)
k=5
# create a vector of fold numbers, repeating 1:k until reaching the number of rows in the data
folds = rep(1:k,
length.out = nrow(d))
# create a new column in the data frame that randomly assigns a fold to each row without replacement
# (so that we don't have a row assigned to two or more folds)
d$fold = sample(folds,
nrow(d),
replace = F)
# view the first few rows of the data frame to see the fold assignments
head(d)
d <- d %>% filter(!is.na(score))
#initialize columns
d[,c('lm', 'lmer')] = NA
# loop through the folds
for (j in 1:k){
cat(j,'') ## print out the progress
## train rows are the ones not in the j-th fold
train.rows = d$fold != j
# test rows are the observations in the j-th fold
test.rows  = d$fold == j
## fit model on training data
## linear regression model
lm1 <- lm(score ~ ha + team + opp, data = d[train.rows,])
## mixed effects linear regression model
lmer1 <- lmer(score ~ ha + (1|team) + (1|opp), data = d[train.rows,])
## make predictions
d$lm[test.rows] = predict(lm1, newdata=d[test.rows,], type='response')
d$lmer[test.rows] = predict(lmer1, newdata=d[test.rows,], type='response')
}
sqrt(mean((d$lm    - d$score)^2, na.rm=T))
sqrt(mean((d$lmer  - d$score)^2, na.rm=T))
dc = readRDS('data/tracts.and.census.with.EV.stations.rds')
dc = dc@data
## create a indicator for whether there is at least
## one lev2 station in each tract
dc = dc %>%
mutate(l2 = ifelse(lev2 > 0 , 1,  0),
l2 = ifelse(is.na(l2), 0, l2))
dc = dc[sample(1:nrow(dc),
5000,
replace = F),]
lm <- lm(l2 ~ log(house.value) + county, data = dc, family = binomial, contrasts = list(county = 'contr.sum'))
lmer <- glmer(l2 ~ log(house.value) + (1|county), data = dc, family = binomial)
coeffs = extract.ranef(lmer, lm)
head(coeffs)
n.obs.county = dc %>%
group_by(county) %>%
summarise(n = n(),
value = mean(l2)) %>%
rename(var = county) %>%
mutate(ranef = 'county')
n.obs.county %>% head()
dff = coeffs %>%
pivot_wider(names_from  = model,
values_from = c(est, se),
names_sep   = '.') %>%
left_join(n.obs.county,
by = c('var', 'ranef'))
head(dff)
tail(dff)
dgg2 = coeffs %>%
filter(ranef == 'county') %>%
arrange(model, est) %>%
mutate(var = factor(var, levels = unique(var))) %>%
left_join(n.obs,
by = c('var', 'ranef'))
g2 = ggplot(dff %>%
filter(ranef == 'county'),
aes(x = est.glm,
y = est.glmer,
label = var))+
geom_abline(slope = 1,
intercept = 0) +
geom_hline(yintercept = 0) +
geom_point(color = pubdarkgray) +
geom_text_repel(hjust = -.2,
color = pubmediumgray) +
labs(title = 'County Coefficients',
x = 'GLM',
y = 'GLMER')
g2 <- ggplot(dff,
aes(x = est.glm,
y = est.glmer,
color = ranef,
label = var)) +
geom_abline(slope = 1,
intercept = 0) +
geom_hline(yintercept = 0) +
geom_segment(aes(xend = est.glm,
y = est.glmer + se.glmer,
yend = est.glmer - se.glmer),
color = pubblue,
linewidth = 0.25,
alpha = 0.3)+
geom_segment(aes(xend = est.glm,
y = est.glm + se.glm,
yend = est.glm - se.glm),
color = pubred,
linewidth = 0.25,
alpha = 0.3) +
geom_point(color = pubblue) +
geom_point(aes(y = est.glm),
color = pubred) +
coord_cartesian(clip='off', expand=F)
g2 %>% pub()
dcc <- dc |> select(l2, county, house.value)
# for reproducibility
set.seed(123)
# set number of folds (usually 5 or 10)
k=5
# create a vector of fold numbers, repeating 1:k until reaching the number of rows in the data
folds = rep(1:k,
length.out = nrow(dcc))
# create a new column in the data frame that randomly assigns a fold to each row without replacement
# (so that we don't have a row assigned to two or more folds)
dcc$fold = sample(folds,
nrow(dcc),
replace = F)
# view the first few rows of the data frame to see the fold assignments
head(dcc)
metrics <- data.frame(fold = 1:k,
logloss1 = NA,
logloss2 = NA)
dcc[,c('lm', 'lmer')] = NA
dcc <- dcc %>% filter(county != "Kodiak Island Borough",
county != "Tuolumne County",
county != "Albemarle County")
for (j in 1:k){
cat(j,'')
train.rows = dcc$fold != j
test.rows  = dcc$fold == j
# the models are already defined in question 9, so only need to predict
dcc$lm[test.rows] = predict(lm, newdata=dcc[test.rows,], type='response')
dcc$lmer[test.rows] = predict(lmer, newdata=dcc[test.rows,], type='response')
}
sqrt(mean((dcc$lm    - dcc$l2)^2, na.rm=T))
sqrt(mean((dcc$lmer  - dcc$l2)^2, na.rm=T))
roc.glm = roc(dcc$l2, dcc$lm)
roc.glmer = roc(dcc$l2, dcc$lmer)
roc.glm$auc
roc.glmer$auc
